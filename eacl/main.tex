\documentclass[11pt]{article}

% review - anonymous, line numbers
% preprint - how it would look
% final - camera ready version
\usepackage[preprint]{acl}

\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{float}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}

\usepackage{pifont}
\newcommand{\cmark}{\textcolor{green}{\ding{51}}} % green check
\newcommand{\xmark}{\textcolor{red}{\ding{55}}} 

\usepackage{listings}
\lstset{basicstyle=\ttfamily,breaklines=true}

\setlength\titlebox{7.5cm}

\title{DogMa: A Dogtag-Based Knowledge Graph Entity Matching Framework}

\author{
    Péter Kardos \\
    University of Szeged \\
    Hungary \\
    \texttt{kardos@inf.u-szeged.hu} \\
    \And
    Máté Vass \\
    University of Szeged \\
    Hungary \\
    \texttt{vassmate@inf.u-szeged.hu} \\
    \AND
    Richárd Farkas \\
    University of Szeged \\
    Hungary \\
    \texttt{rfarkas@inf.u-szeged.hu} \\
    \And
    Miklós Krész \\
    InnoRenew CoE \\
    University of Primorska \\
    Slovenia \\
    \texttt{miklos.kresz@innorenew.eu} \\
}

\begin{document}

\maketitle

\begin{abstract}

\colorbox{pink}{TODO}

\end{abstract}

\section{Introduction}

Knowledge Graphs (KGs) are semi-structured knowledge bases in which nodes are typically associated with long textual descriptions, while the textual labels of edges semantically characterize the relationships among nodes. Processing KGs therefore requires specialized methodologies: standard graph mining techniques, such as Graph Neural Networks \cite{gnn_1, gnn_2}, generally ignore textual information, whereas natural language processing (NLP) methods, including Retriever-Augmented Generation (RAG) \cite{rag_survey, graphrag}, are designed for purely textual data.

We study the task of \emph{Knowledge Graph Entity Matching}, where the goal is to identify nodes across two domain-specific KGs that refer to the same real-world entity. These matches enable KG fusion, which is particularly valuable in specialized industrial settings. We note that KG entity matching differs from the well-studied entity alignment problem \cite{alignment_survey}. In KG entity matching, the textual information of both KGs is in the same language, and only a limited number of nodes are expected to match \cite{matching_survey}, in contrast to entity alignment settings.

Notably, both tasks can benefit from embedding-based approaches, where entities, relations, and even schemas are represented in continuous vector spaces. In their seminal paper, \citet{sentence_bert} demonstrate how Sentence-BERT embeddings can capture semantic similarity between textual representations of entities, helping both alignment and matching in knowledge graphs.

Entity matching is challenging: while simple string matching offers a strong baseline, identical names can denote different entities, and equivalent entities often appear under different names.

Motivated by advances in Large Language Models (LLMs), we formulate KG entity matching as a RAG-like problem. Our main contribution is the design of node-level textual representations that combine textual descriptions with local graph context. These representations are used to retrieve candidate nodes from the other KG, after which an LLM selects the best match, if any.

Experiments on the \emph{OAEI KG Matching Benchmark } \cite{oaei_kgtrack} show that LLMs effectively leverage semi-structural information, though challenges remain for them. Compared to traditional embedding-based methods, LLMs achieve higher accuracy, yet the performance gap is smaller than expected. Our contributions are threefold:

\begin{enumerate}[nosep]
    \setlength\itemsep{0em}
    \setlength\parskip{0em}
    \setlength\parsep{0em}
    \item We propose multiple textual representation strategies for encoding semi-structured KG information.
    \item We conduct extensive experiments on KG entity matching, showing that LLMs achieve strong accuracy but also make unexpected errors.
    \item We demonstrate that lightweight, resource-efficient approaches perform only slightly worse than LLMs.
\end{enumerate}

\section{Literature review}

Knowledge Graphs (KGs) have emerged as a powerful paradigm for representing structured knowledge across various domains. They capture entities, attributes, and relations in a graph structure, enabling semantic search, reasoning, and interoperability. As highlighted in \citet{kg_opportunities}, KGs offer significant opportunities but also face challenges in terms of scalability, heterogeneity, and alignment across different sources.

Real-world applications may require the integration of heterogeneous KGs, since datasets frequently differ in scope, granularity, and structure. Aligning knowledge graphs is crucial for achieving semantic interoperability: it allows entities and relations that represent the same real-world concept to be connected, unlocking richer insights and applications, for example in healthcare \cite{hl7_fhir, personalized_medicine} or logistics \cite{logistics_alignment}. It is important to note that this task differs from recent \emph{GraphRAG} approaches \cite{graphrag}, which focus on retrieval-augmented generation using graph-structured contexts. In our setting, the graphs themselves are already present, and the challenge lies in effectively aligning them.

The alignment problem has historically been studied under the umbrella of \emph{Ontology Alignment}, which typically assumes entire ontologies of similar size and structure. Ontology alignment methods emphasize schema-level matching, class hierarchies, and conceptual mappings \cite{alignment_survey}. While early works focused on fine-tuning embedding-based approaches \cite{alignment_bertmap, alignment_embedding}, researchers started to exploit LLMs to achieve better alignment results \cite{alignment_llm_1, alignment_llm_2, alignment_llm_3}. Since the problem is still quite challenging in real-world scenarios, the role of additional external background knowledge has also been studied recently \cite{survey_background}.

On the other hand, \emph{Entity Matching} focuses on aligning entities across heterogeneous graphs that may differ significantly in size, density, and coverage \cite{matching_survey}. The two tasks overlap but differ in scope: ontology alignment primarily addresses schema alignment, while entity matching deals with instance-level entity correspondence. Emerging approaches include optimizing traditional embedding-based solutions \cite{matching_embedding}, or applying novel methods such as transformers \cite{matching_transformers} or Monte Carlo Tree Search \cite{matching_montecarlo}.

Despite all other attempts, the extensive use of LLMs still remains one of the main directions in the field of KG entity matching \cite{matching_chatgpt_survey, matching_llm_survey}. Existing studies focus on different aspects of harnessing the capabilities of large language models, such as zero-shot prompting \cite{matching_zeroshot}, merging the results of different decision strategies \cite{matching_multi_decision}, utilizing extensive background knowledge \cite{matching_background} or fine-tuning existing models for the task \cite{matching_finetune}.

\emph{Context Engineering} has emerged as a critical discipline in optimizing LLM performance through systematic information payload design \cite{context_survey}. Recent works showcase various approaches to context engineering: from structured knowledge representations \cite{jiang2023structgpt,fang2024karpa} to iterative reasoning frameworks \cite{luo2023reasoning_rog,sun2023think} that combine LLMs with graph-based methods. Studies show that the format and organization of input context significantly impacts LLM performance, with some works like \cite{liu2024_struxgpt} suggesting that well-structured, hierarchical representations often lead to more accurate responses. The field encompasses diverse strategies including knowledge verbalization \cite{ankush-etal-2023-kitlm}, triple-to-text conversion \cite{agarwal-etal-2021-knowledge}, and various data representation formats (natural language, JSON, Python code). While some research indicates that more structured formats (e.g., Python) outperform natural language representations, findings remain context-dependent. Furthermore, the ordering of information (as shown in \cite{ge2025graphdescriptiveorderaffect}) can substantially affect LLM reasoning capabilities, highlighting the importance of careful context design in LLM applications.

In their work, \citet{compare_1} identify candidate alignments for a given entity by considering name and structure embedding similarities between entities across the KG pairs. Then, an LLM makes iterative decisions for the given candidate set until it converges to a final answer. In a more recent study, \citet{compare_2} use LLMs to summarize entity descriptions and select candidates based on the embeddings of these summaries. The ultimate decision in their approach is similarly determined by an LLM.

Building on these ideas, we extend the scope of our work to match entities on highly heterogeneous knowledge graphs and explore context engineering to improve performance. We examine different types of entity descriptions, traditional methods and prompting techniques to optimize entity matching performance. Interestingly, our results show that traditional embedding-based methods can achieve performance nearly on par with LLM-based approaches, highlighting that simpler methods remain highly competitive in certain scenarios. We further highlight that the effectiveness of different methods depends on the type of entity descriptions used.

\section{Dataset}
\label{sec:dataset}

\begin{table*}[h]
\centering
\resizebox{\textwidth}{!}{%

\begin{tabular}{llccc}
\toprule
\textbf{Source} & \textbf{Source URL} & \textbf{\#Instances} & \textbf{\#Properties} & \textbf{\#Classes} \\
\midrule
Star Wars Wiki                 & \url{http://starwars.wikia.com}                & 145,033 & 700 & 269 \\
The Old Republic Wiki          & \url{http://swtor.wikia.com}                   & 4,180   & 368 & 101 \\
Star Wars Galaxies Wiki        & \url{http://swg.wikia.com}                     & 9,634   & 148 & 67  \\
\midrule
Marvel Database                & \url{http://marvel.wikia.com}                  & 210,996 & 139 & 186 \\
Marvel Cinematic Universe Wiki & \url{http://marvelcinematicuniverse.wikia.com} & 17,187  & 147 & 55  \\
\midrule
Memory Alpha                   & \url{http://memory-alpha.wikia.com}            & 45,828  & 325 & 181 \\
Star Trek Expanded Universe    & \url{http://stexpanded.wikia.com}              & 13,426  & 202 & 283 \\
Memory Beta                    & \url{http://memory-beta.wikia.com}             & 51,323  & 423 & 240 \\
\bottomrule
\end{tabular}
}
\vspace{-6px}
\caption{The scraped fandom sites and their graph's sizes}
\label{tab:datasets}
\end{table*}


\begin{table}[h]
\centering

\begin{tabular}{l|c|}
%\cline{2-2}
%                                             & \multicolumn{1}{l|}{\textbf{Count}} \\ 
\hline
\multicolumn{1}{|l|}{mcu-marvel}             & 1667                                \\ \hline
\multicolumn{1}{|l|}{memoryalpha-memorybeta} & 9365                                \\ \hline
\multicolumn{1}{|l|}{stexpanded-memoryalpha} & 1779                                \\ \hline
\multicolumn{1}{|l|}{swg-starwars}           & 1121                                \\ \hline
\multicolumn{1}{|l|}{swtor-starwars}         & 1429                                \\ \hline
\end{tabular}
\vspace{-6px}
\caption{Number of gold pairs}
\label{tab:golds}
\end{table}

The \emph{Ontology Alignment Evaluation Initiative (OAEI)} has provided community-driven benchmarks for decades. The \emph{OAEI Knowledge Graph Track}\footnote{Data: \url{https://oaei.ontologymatching.org/2025/knowledgegraph}} has focused specifically on entity alignment across large-scale heterogeneous KGs, making it a critical reference point for evaluating progress in entity matching.

This benchmark consists of eight fandom wiki sites (see Table \ref{tab:datasets}), provided in \textit{OWL} format, covering popular universes such as \textit{Star Wars}, \textit{Star Trek}, and \textit{Marvel}. The graphs span a variety of domains (e.g., games, movies, comics) and vary in size from approximately 5,000 to 200,000 nodes. Each graph is derived from a site scrape that includes the infobox information, primarily textual attributes, the first few paragraphs of the main article text, every entity mention in the whole page and links to images.

Manually annotated reference alignments are available for five graph pairs (See Table \ref{tab:golds}). However, due to the limited annotation resources, these alignments are partial: not all correct correspondences are included in the gold standard. As a result, the evaluation requires special handling. Specifically, when assessing a predicted correspondence, if neither node in the pair appears in the gold standard, the status of the pair remains indeterminate (i.e., it may be either a true match or a false positive). Such pairs are excluded from the evaluation. Precision, recall, and F1 scores are then computed on the remaining pairs to assess system performance. This evaluation aligns with the dataset creators' method.

\section{Methodology}

\begin{figure*}[t] % "H" keeps the image exactly here (requires float package)
    \centering
    \includegraphics[width=1\textwidth]{latex/images/DogtagExtraction_orange2.drawio.png}

    \vspace{-10px}
    \caption{Information used from the KG to create the representations}
    \label{fig:dogtag_extract}
\end{figure*}

\begin{figure*}[t] % "H" keeps the image exactly here (requires float package)
    \centering
    \includegraphics[width=1\textwidth]{latex/images/dogma_matcher.drawio.png}
    \vspace{-22px}
    \caption{Proposed KG entity matching framework}
    \label{fig:framework}
\end{figure*}

In this section, we describe our pipeline for the KG entity matching problem. Our methodology consists of four major components: (i) generation of entity text representations (dogtags), (ii) similarity-based and model-based matching algorithms, (iii) deduplication to enforce 1:1 alignments and (iv) merging multiple pairings into a single one. The framework diagram, shown in Figure \ref{fig:framework}, illustrates the overall process of these stages.

\subsection{Textual Entity Representations (Dogtags)}

Entities in the fandom KGs contain rich textual information. To leverage them in a controlled manner, we construct multiple forms of entity text representations, which we refer to as dogtags:

\begin{itemize}[nosep]
    \setlength\itemsep{0em}
    \setlength\parskip{0em}
    \setlength\parsep{0em}
    \item \textbf{Node:} Includes only the entity’s label, alternative labels, type, and abstract.
    \item \textbf{Node - Short:} Same as \emph{Node}, but only the first two sentences of the abstract are kept.
    \item \textbf{Node + Relations:} Contains all available textual and relational information for each node, excluding links to other fandom pages, images or irrelevant metadata.
    \item \textbf{LLM Summary:} A concise paragraph generated with an LLM, leveraging all available textual and relational information. To maintain tractability for LLM input, we limit relation neighbors to a maximum of 20.
\end{itemize}

We visualized the dogtag extraction process in Figure \ref{fig:dogtag_extract}
Each entity in every graph is associated with all four dogtags, which serve as inputs to the matching algorithms described below.

\subsection{Matching Algorithms}

All dogtags are embedded into dense vector representations using a Sentence Embedder model. We measure semantic similarity between entities using cosine similarity. We evaluate several strategies to extract alignments where we always start the matching from the smaller graph's side:

\begin{enumerate}[nosep]
    \setlength\itemsep{0em}
    \setlength\parskip{0em}
    \setlength\parsep{0em}
    \item \textbf{Exact Matching:} If the label or an alternative label matches exactly across the two graphs, the corresponding pair is aligned. This method is independent of the dogtags.
    \item \textbf{Top-1 Embedding Similarity:} For each entity in Graph A, we identify the entity in Graph B with the highest cosine similarity score and align them.
    \item \textbf{Top-10 with Reranking:} For each entity in Graph A, we retrieve the top-10 most similar candidates from Graph B. We then apply a Reranker Model to reorder the candidates and select the highest ranking result.
    \item \textbf{LLM-based Selection:} We start from the top-10 most similar candidates. We present the anchor entity alongside its candidates to an LLM, asking it to select the best match or \textit{None} if no appropriate match exists. We test two prompting styles:
    \begin{itemize}
        \item \textbf{Normal:} The task is explained in a straightforward manner, asking the model to choose the most appropriate candidate.  
        \item \textbf{Strict:} The instructions emphasize that a candidate should only be selected when the model is completely certain of a match.
    \end{itemize}
\end{enumerate}

Methods (2, 3, 4) are applied separately for each dogtag representation, allowing us to analyze the impact of different text extractions on matching performance.

\subsection{Deduplication}

The above methods may yield 1:N or N:M pairs, while we require 1:1 alignments. We call these duplicates. To resolve duplicates, we apply two complementary strategies:

\textbf{Score-based Deduplication} (applied to Methods 2 and 3): When similarity or reranker scores are available, we retain only the pair with the highest score for each entity.

\textbf{String-based Deduplication} (applied to Methods 1 and 4): When explicit scores are not available, we rely on fuzzy string matching between node names to select the most appropriate pair.

\subsection{Merging Alignments}
\label{sec:merging_alignments}

As we obtain multiple sets of alignments from these methods, we are interested in integrating them into a single alignment. The input for our procedure is two alignment sets: a primary alignment set, which has higher priority (e.g., produced by a more reliable method), and a secondary alignment set, which serves as a supplementary source of matches.

When the primary method identifies exact matches (nodes with corresponding labels), we additionally compute a threshold defined as the median of their similarity scores. This threshold serves as an extra criterion in the merging process, ensuring that only secondary pairs with sufficiently strong similarity are considered for inclusion.

For each pair in the secondary alignment set, we check whether either entity is already aligned in the primary set. If not, we further verify that the pair’s similarity meets or exceeds the previously computed median threshold; only then is the pair added to the merged alignment. Pairs that are already aligned or fall below the threshold are discarded to avoid duplicates or weak matches.

This approach ensures that alignments remain strictly 1:1 across graphs while allowing complementary methods to contribute additional matches. With this approach we can study how different alignment strategies reinforce each other, and measure their incremental contribution to overall coverage.

\section{Experimental setup}
%We evaluate three systems: (i) a simple embedding-based baseline that selects the most similar candidate (``Top1''), 
%(ii) another baseline that retrieves the top-10 candidates by embedding similarity and applies a reranker model to select the top one (`Top10 Reranked''), 
%and (iii) our proposed approach, where an LLM is prompted with the top-10 candidates and selects the correct match or abstains. For this solution we have a Normal and Strict version.
%The first two serve as baselines, while the third represents our framework.

\begin{table*}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{ccc|ccc|ccc|ccc|ccc|ccc}
\toprule
\textbf{Mode} & \textbf{Dogtag} & \textbf{Method} 
& \multicolumn{3}{c|}{mcu-marvel} 
& \multicolumn{3}{c|}{memoryalpha-memorybeta} 
& \multicolumn{3}{c|}{stexpanded-memoryalpha} 
& \multicolumn{3}{c|}{swg-starwars} 
& \multicolumn{3}{c}{swtor-starwars} \\
\cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-12} \cmidrule(lr){13-15} \cmidrule(lr){16-18}
 &  &  
 & \textbf{Precision} & \textbf{Recall} & \textbf{F1} 
 & \textbf{Precision} & \textbf{Recall} & \textbf{F1} 
 & \textbf{Precision} & \textbf{Recall} & \textbf{F1} 
 & \textbf{Precision} & \textbf{Recall} & \textbf{F1} 
 & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
Base & Short2S & Top1 & 0.4157 & 0.4097 & 0.4127 & 0.7932 & 0.7369 & 0.7640 & 0.8434 & 0.8415 & 0.8424 & 0.7953 & 0.7520 & 0.7730 & 0.8683 & 0.8579 & 0.8631 \\
Base & Short2S & Top100 Reranked & 0.4666 & 0.4229 & 0.4437 & 0.7751 & 0.6988 & 0.7350 & 0.7069 & 0.6914 & 0.6991 & 0.7541 & 0.6896 & 0.7204 & 0.8365 & 0.8020 & 0.8189 \\
Base & Short2S & Top10 LLM NotStrict & 0.6810 & 0.4265 & 0.5245 & 0.9566 & 0.8119 & 0.8783 & 0.9306 & 0.8746 & 0.9018 & 0.9097 & 0.7636 & 0.8303 & 0.9414 & 0.8775 & 0.9084 \\
Base & Short2S & Top10 LLM Strict & 0.7149 & 0.3731 & 0.4903 & 0.9639 & 0.7845 & 0.8650 & 0.9296 & 0.8606 & 0.8938 & 0.9116 & 0.7083 & 0.7972 & 0.9387 & 0.8565 & 0.8957 \\
Base & Short & Top1 & 0.4069 & 0.3959 & 0.4013 & 0.7248 & 0.6492 & 0.6849 & 0.6955 & 0.7010 & 0.6982 & 0.8034 & 0.7583 & 0.7802 & 0.8281 & 0.7992 & 0.8134 \\
Base & Short & Top100 Reranked & 0.4421 & 0.4121 & 0.4266 & 0.7661 & 0.6842 & 0.7228 & 0.6981 & 0.6863 & 0.6922 & 0.7431 & 0.6967 & 0.7192 & 0.8498 & 0.8076 & 0.8281 \\
Base & Short & Top10 LLM NotStrict & 0.6650 & 0.3989 & 0.4987 & 0.9326 & 0.7630 & 0.8393 & 0.9081 & 0.8336 & 0.8693 & 0.9170 & 0.7788 & 0.8423 & 0.9405 & 0.8635 & 0.9004 \\
Base & Short & Top10 LLM Strict & 0.6938 & 0.3533 & 0.4682 & 0.9426 & 0.7430 & 0.8310 & 0.9168 & 0.8302 & 0.8714 & 0.9213 & 0.7315 & 0.8155 & 0.9418 & 0.8488 & 0.8929 \\
Base & Long & Top1 & 0.3740 & 0.3215 & 0.3458 & 0.8071 & 0.7555 & 0.7805 & 0.7800 & 0.7774 & 0.7787 & 0.7976 & 0.7698 & 0.7835 & 0.8883 & 0.8628 & 0.8754 \\
Base & Long & Top100 Reranked & 0.3506 & 0.2837 & 0.3137 & 0.7567 & 0.6886 & 0.7210 & 0.7346 & 0.7128 & 0.7235 & 0.7692 & 0.6985 & 0.7321 & 0.8591 & 0.8237 & 0.8410 \\
Base & Long & Top10 LLM NotStrict & 0.6117 & 0.3515 & 0.4465 & 0.9559 & 0.8590 & 0.9049 & 0.9669 & 0.9028 & 0.9337 & 0.9331 & 0.7957 & 0.8589 & 0.9443 & 0.9013 & 0.9223 \\
Base & Long & Top10 LLM Strict & 0.6245 & 0.2993 & 0.4047 & 0.9615 & 0.8502 & 0.9024 & \textbf{0.9720} & 0.8960 & 0.9324 & 0.9377 & 0.7654 & 0.8428 & \textbf{0.9504} & 0.8845 & 0.9163 \\
Base & Summary & Top1 & 0.4556 & 0.4277 & 0.4412 & 0.9183 & 0.8947 & 0.9064 & 0.9184 & 0.9168 & 0.9176 & 0.8791 & 0.8760 & 0.8776 & 0.8915 & 0.8852 & 0.8883 \\
Base & Summary & Top100 Reranked & 0.3918 & 0.3431 & 0.3658 & 0.7773 & 0.7187 & 0.7469 & 0.7803 & 0.7527 & 0.7662 & 0.6816 & 0.5825 & 0.6282 & 0.7818 & 0.7348 & 0.7576 \\
Base & Summary & Top10 LLM NotStrict & 0.5289 & 0.4505 & 0.4866 & 0.9546 & 0.8690 & 0.9098 & 0.9522 & 0.9174 & 0.9344 & 0.9344 & 0.8011 & 0.8626 & 0.9076 & 0.8656 & 0.8861 \\
Base & Summary & Top10 LLM Strict & 0.5227 & 0.4139 & 0.4620 & 0.9583 & 0.8279 & 0.8884 & 0.9516 & 0.8949 & 0.9224 & 0.9408 & 0.7368 & 0.8264 & 0.9032 & 0.8425 & 0.8718 \\
\hline
& ExactMatch &  & 0.8839 & 0.6623 & 0.7572 & 0.9449 & 0.8823 & 0.9125 & 0.9659 & 0.9089 & 0.9366 & 0.9367 & 0.6200 & 0.7461 & 0.9420 & 0.8866 & 0.9135 \\
\hline
Union & Short2S & Top1 & 0.7919 & 0.6941 & 0.7398 & 0.9407 & 0.8983 & 0.9190 & 0.9637 & 0.9264 & 0.9447 & 0.9282 & 0.7154 & 0.8081 & 0.9412 & 0.9069 & 0.9237 \\
Union & Short2S & Top100 Reranked & 0.8466 & 0.6719 & 0.7492 & 0.9217 & 0.8930 & 0.9071 & 0.8879 & 0.9213 & 0.9043 & 0.8495 & 0.7047 & 0.7704 & 0.9238 & 0.8992 & 0.9113 \\
Union & Short2S & Top10 LLM NotStrict & 0.8732 & 0.6941 & 0.7734 & 0.9351 & 0.9197 & 0.9273 & 0.9197 & 0.9404 & 0.9300 & 0.9000 & 0.8109 & 0.8531 & 0.9267 & 0.9202 & 0.9235 \\
Union & Short2S & Top10 LLM Strict & 0.8798 & 0.6719 & 0.7619 & 0.9377 & 0.9122 & 0.9248 & 0.9169 & 0.9297 & 0.9232 & 0.9018 & 0.7698 & 0.8306 & 0.9258 & 0.9167 & 0.9212 \\
Union & Short & Top1 & 0.7895 & 0.6929 & 0.7380 & 0.9381 & 0.9010 & 0.9192 & 0.9566 & 0.9297 & 0.9430 & 0.9300 & 0.7226 & 0.8133 & 0.9417 & 0.9041 & 0.9225 \\
Union & Short & Top100 Reranked & 0.8505 & 0.6725 & 0.7511 & 0.9151 & 0.8921 & 0.9035 & 0.8818 & 0.9224 & 0.9016 & 0.8543 & 0.6958 & 0.7670 & 0.9246 & 0.9006 & 0.9124 \\
Union & Short & Top10 LLM NotStrict & 0.8717 & 0.6929 & 0.7721 & 0.9247 & 0.9224 & 0.9235 & 0.9131 & 0.9449 & 0.9287 & 0.9058 & 0.8234 & 0.8626 & 0.9275 & 0.9216 & 0.9245 \\
Union & Short & Top10 LLM Strict & 0.8772 & 0.6731 & 0.7617 & 0.9288 & 0.9170 & 0.9229 & 0.9159 & 0.9432 & 0.9294 & 0.9098 & 0.7921 & 0.8469 & 0.9285 & 0.9174 & 0.9229 \\
Union & Long & Top1 & 0.8116 & 0.6821 & 0.7412 & 0.9399 & 0.9045 & 0.9218 & 0.9611 & 0.9309 & 0.9457 & 0.9282 & 0.7154 & 0.8081 & 0.9403 & 0.9034 & 0.9215 \\
Union & Long & Top100 Reranked & 0.8065 & 0.6725 & 0.7334 & 0.9133 & 0.8961 & 0.9046 & 0.9306 & 0.9354 & 0.9330 & 0.8952 & 0.7083 & 0.7908 & 0.9395 & 0.9020 & 0.9204 \\
Union & Long & Top10 LLM NotStrict & 0.8644 & 0.6809 & 0.7617 & 0.9316 & 0.9350 & 0.9333 & 0.9536 & 0.9590 & 0.9563 & 0.9226 & 0.8296 & 0.8736 & 0.9395 & 0.9342 & 0.9368 \\
Union & Long & Top10 LLM Strict & 0.8718 & 0.6731 & 0.7596 & 0.9335 & 0.9322 & 0.9328 & 0.9551 & 0.9567 & 0.9559 & 0.9248 & 0.8118 & 0.8646 & 0.9419 & 0.9300 & 0.9359 \\
Union & Summary & Top1 & 0.8681 & 0.7067 & 0.7791 & 0.9409 & 0.9073 & 0.9238 & 0.9624 & 0.9354 & 0.9487 & 0.9316 & 0.7163 & 0.8099 & 0.9404 & 0.9055 & 0.9226 \\
Union & Summary & Top100 Reranked & 0.8242 & 0.7001 & 0.7571 & 0.9209 & 0.9036 & 0.9121 & 0.9381 & 0.9286 & 0.9333 & 0.9009 & 0.6976 & 0.7863 & 0.9342 & 0.9041 & 0.9189 \\
Union & Summary & Top10 LLM NotStrict & 0.8262 & 0.7870 & 0.8061 & 0.9293 & 0.9364 & 0.9328 & 0.9483 & 0.9494 & 0.9489 & 0.9206 & 0.8582 & 0.8883 & 0.9367 & 0.9321 & 0.9344 \\
Union & Summary & Top10 LLM Strict & 0.8329 & 0.7624 & 0.7961 & 0.9315 & 0.9322 & 0.9318 & 0.9473 & 0.9505 & 0.9489 & 0.9254 & 0.8305 & 0.8754 & 0.9353 & 0.9300 & 0.9326 \\
\bottomrule
\end{tabular}
}
\vspace{-6px}
\caption{Performance comparison across fandom domains, dogtags and methods}
\label{tab:performance}
\end{table*}

\paragraph{Models.} 
For embeddings we use \emph{BAAI/bge-large-en-v1.5}, for reranking we use \emph{BAAI/bge-reranker-large}, 
and as the LLM we use \emph{Llama-3.1-70B-Instruct}, all obtained from HuggingFace. 
Sampling is disabled for the LLM to ensure deterministic outputs. 

\paragraph{Datasets and Evaluation.} 
We follow the evaluation protocols recommended by the dataset creators (see Section \ref{sec:dataset})

\paragraph{Implementation Details.} 
All methods run purely in inference mode; no finetuning or additional training is performed.
The experiments are executed on 4 × H100 GPUs. 
Since all models and inference steps are deterministic, results are stable across repeated runs. 

\paragraph{Reproducibility.} 
Our full codebase is publicly available on GitHub.\footnote{\url{https://github.com/material1999/KGEntityMatching-Public}} 
We provide a \texttt{commands.txt} file that documents the exact commands used for running every experiment end-to-end.

\paragraph{Experiments}

Table~\ref{tab:performance} presents the results for the KG entity matching task. 
We evaluate all systems in combination with the exact matching pairs. For the Top1 and Reranked baselines, we apply the median thresholding strategy described in Section~\ref{sec:merging_alignments}. 
In contrast, when merging with the LLM-based selection, no thresholding is applied since the model does not produce scores; however, the LLM may abstain from selecting a pair.

%In table \#TODO, we depict the results for KG Matching task and compare the performance of our proposed framework. To embed the individual text descriptions we used \emph{BAAI/bge-large-en-v1.5}, for the reranker model we used \emph{BAAI/bge-reranker-large}, as for the LLM we picked \emph{Llama-3.1-70B-Instruct} where we turned off sampling to make the generation deterministic. All of these models we got from Huggingface. 
%To maintain tractability for the LLM Summary, the relations were sampled when exceeding 20 neighbors.
%To run the experiments we used 4×H100 GPUs. 
%All of our code available on github\footnote{Github: \url{https://github.com/material1999/KGEntityMatching-Public/tree/main}} including a commands.txt which goes through the whole pipeline.


\section{Discussion}

\paragraph{How consistent is the LLM in candidate selection?}
To assess the stability of the LLM-based selection, we investigated how sensitive its choices are to the ordering of candidate entities. Specifically, we varied the order of the top-10 most similar candidates (based on cosine similarity) presented to the LLM and compared the resulting alignments. We considered three settings:
\begin{itemize}[nosep]
\setlength\itemsep{0em}
\setlength\parskip{0em}
\setlength\parsep{0em}
\item \textbf{Original:} candidates sorted in decreasing cosine similarity,
\item \textbf{Reverse:} candidates sorted in the opposite order,
\item \textbf{Random:} candidates presented in a randomly permuted order.
\end{itemize}

Figure~\ref{fig:shuffle_llm_pick} shows the distribution of the positions chosen by the LLM. In the original ordering, most selections concentrate on the first candidate, as expected. Reversing the order flips this distribution, with the majority now drawn from the end of the original ranking. Interestingly, the random ordering reveals a non-uniform bias: the LLM strongly prefers earlier positions, with selection probability decreasing as candidates appear later in the list. This effect persisted across three independent random shuffles, confirming its robustness. Notably, even in the reversed ordering, we observe a small but systematic preference for the first position, suggesting a positional bias beyond semantic content.

To quantify the impact of these orderings, we compared the resulting alignments against those obtained with the original order (Figure~\ref{fig:shuffle_size_intersect}). The overlap is limited: the reversed ordering retains only about 50\% of the original pairs, with an overall 33\% lower selection rate. The random ordering performs slightly better, preserving 66\% of the pairs, but still diverges substantially. These findings indicate that the LLM’s alignment decisions are highly sensitive to candidate ordering and subject to positional bias. Consequently, LLM-based selection cannot be considered order-invariant, and care must be taken when designing candidate presentation strategies.


\begin{figure*}[h] % "H" keeps the image exactly here (requires float package)
    \centering
    \includegraphics[width=1\textwidth]{latex/images/shuffle_top10_llmselect_order.png}
    \vspace{-30px}
    \caption{LLM pick index frequency based on different orderings of the Top10 entities.}
    \label{fig:shuffle_llm_pick}
\end{figure*}


\begin{figure}[h]
    \includegraphics[width=1.0\columnwidth]{latex/images/shuffle_size_intersection.png}
    \vspace{-30px}
    \caption{Top10 Ordering: Size and intersection comparison. (The intersections are compared agains the cosine similarity ordering.)}
    \label{fig:shuffle_size_intersect}
\end{figure}


\paragraph{Why does the LLM summary perform better?}

\begin{figure}[h]
    \includegraphics[width=1.0\columnwidth]{latex/images/gold_recall_top10.png}
    \vspace{-25px}
    \caption{Top10 pairs' Gold Recall for each Dogtag version.}
    \label{fig:gold_recall}
\end{figure}

We made a comparison over the Top10 elements and the gold pairs in terms of recall (How many gold pairs are present in the Top10). We compared all 4 of our Dogtag setups in this manner. In figure \ref{fig:gold_recall} we show our findings. We can see that there are more gold pairs present when using the LLM generated summary than any other method. This can be the root cause why all the underlying matchers perform better when using this textual representation of an entity.

There are two key aspects here:
\begin{itemize}[nosep]
    \setlength\itemsep{0em}
    \setlength\parskip{0em}
    \setlength\parsep{0em}
    \item Wiki Links are very helpful
    \item Natural language text is better then structured text
\end{itemize}

On the Wiki Links, we manually looked into what additional information do these relations bring to the table. We came to the conclusion that these are great for an extension but the most important information is already present in the previously included relations/text. Our conclusion is that the wiki links cannot make that much of a difference. We show examples in Appendix \ref{sec:examples} where the newly brought in informations is barely visible on the LLM Summary texts. All things point to the Summary text being better quality than the structured text. In the Appendix examples it is visible that multiple relations have the same information, meaning redundancy, which in case relevant information is duplicated it can shift the embeddings in the opposite directions from the gold pairs. 
\#TODO


We included examples in Table \ref{tab:example-comparison}. 


\paragraph{Did the LLM Summary include information outside the prompt?}

No! We manually checked hundreds of LLM summaries and found no information that was generated outside the prompt's information set.  


\colorbox{pink}{TODO}

\section*{Conclusion}

\colorbox{pink}{TODO}

\section*{Limitations}
We observed that some of the gold-standard pairs provided for evaluation were incorrect, which may have affected performance estimates. Moreover, the Top-1 and Reranking workflows are designed to always output a match, even when no true correspondence exists. While this was not problematic in our evaluation setup, it would represent a critical flaw in applications where the full set of gold pairs is known. LLM-based solutions, although often more accurate, require substantially more computational resources, which limits their accessibility to many users. 
Additionally, entities with sparse or noisy descriptions/relations are harder to align reliably. Embedding-based methods may conflate entities with highly similar contexts (e.g., multiple characters sharing titles or roles), leading to false positives. Finally, our deduplication heuristics can discard valid alignments if similarity scores fail to reflect the true underlying connections.

\colorbox{pink}{TODO}

\section*{Acknowledgments}
This research has been supported by the European Union project RRF-2.3.1-21-2022-00004 within the framework of the Artificial Intelligence National Laboratory. Miklós Krész was supported by the Slovenian Research and Innovation Agency (ARIS) through research program P1-0404 and by the research program Cognicom (0013103) at the University of Primorska. He is also grateful for the support of ARIS grant J7-60122.

\bibliography{custom}

\appendix

\section{Examples}
\label{sec:examples}

We illustrate different Dogtag representations for the entity:
\texttt{Black\_Sun\_AEG-77\_Vigo\_Gunship}.  

\# TODO:Can you formulate this into an appendix part of an ACL paper?

Node
label	Black Sun AEG-77 Vigo Gunship
altLabel	Black Sun AEG-77 Vigo Gunship
type	class/starship
abstract	The Black Sun's transports became a common sight in both the underworld and legitimate shipping with the advent of Xizor Transport Systems. This particular transport class became well known as lightly defended. Pirates from a variety of bands began picking off the freighters, particularly those smuggling illegal cargo for Black Sun. Instead of dedicating fighters to escort transports carrying valuable cargo, Black Sun began modifying some of their transports to carry heavy firepower and act as escorts for the rest of the convoys. Named after Black Sun's lieutenants, the Vigo, this once-merchant class vessel has been elevated to the status of "gunship."
This gunship comes equipped with two forward mounted guns, four top mounted turrets, and two bottom mounted turrets. The gunship, originally a transport ship, is still large enough to hold a number of items and passengers inside. Black Sun AEG-77 Vigo Gunship Chassis Blueprints


|||||

Node - Short
label	Black Sun AEG-77 Vigo Gunship
altLabel	Black Sun AEG-77 Vigo Gunship
type	class/starship
abstract	The Black Sun's transports became a common sight in both the underworld and legitimate shipping with the advent of Xizor Transport Systems. This particular transport class became well known as lightly defended.


|||||

Node + Relation
abstract	The Black Sun's transports became a common sight in both the underworld and legitimate shipping with the advent of Xizor Transport Systems. This particular transport class became well known as lightly defended. Pirates from a variety of bands began picking off the freighters, particularly those smuggling illegal cargo for Black Sun. Instead of dedicating fighters to escort transports carrying valuable cargo, Black Sun began modifying some of their transports to carry heavy firepower and act as escorts for the rest of the convoys. Named after Black Sun's lieutenants, the Vigo, this once-merchant class vessel has been elevated to the status of "gunship."This gunship comes equipped with two forward mounted guns, four top mounted turrets, and two bottom mounted turrets. The gunship, originally a transport ship, is still large enough to hold a number of items and passengers inside. Black Sun AEG-77 Vigo Gunship Chassis Blueprints
accel	10
altLabel	['Black Sun AEG-77 Vigo Gunship', 'Blacksun AEG-77 Vigo Gunship']
astromech	Flight computer
certification	Flight\_Artistry\_(Skill)
crew	Pilot , co-pilot/operations officer, Engineer, gunner x6
decel	10
label	Black Sun AEG-77 Vigo Gunship
mass	~3 million
name	AEG-77
pitch	10
role	Multipassenger ship, Transporter, Battleship, Escort
roll	10
speed	0.6
subject	Category:Freelance\_ships
type	starship
weapons	2
yaw	10

|||||

LLM Summary
 The Black Sun AEG-77 Vigo Gunship is a modified transport vessel that has been converted into a gunship to escort convoys carrying valuable cargo. Originally a lightly defended freighter, it has been equipped with heavy firepower, including two forward-mounted guns and six turrets. The gunship can hold multiple passengers and items, and is crewed by a pilot, co-pilot, engineer, and six gunners. With a mass of approximately 3 million units and a speed of 0.6, the AEG-77 is a formidable escort vessel in the Black Sun fleet.





These are different representations for the same entity. I want to show this example. What would be the best way to format it into an appendix section called "Examples"? I have a 2 column type latex document. 

If you put it in a table I want the first column to be rotated text so that the second column have bigger space. Include all the 4 variants! Make the rotate not collide with the other column



\begin{table*}[h!]
\footnotesize
\centering
\begin{tabular}{|p{7cm}|p{7cm}|}
\hline
\textbf{http://swg.wikia.com/A\_Demon\_Doll} & 
\textbf{http://starwars.wikia.com/Desert\_Demon\_Doll} \\
\hline

\textbf{LLM Summary: \cmark} \newline
A Demon Doll is a reward item in the game, specifically obtained by completing the Tusken Valuables collection. It is categorized as a collection reward and has a corresponding wiki page with additional information and images, including a picture of the Tusken doll. The item is also linked to the Tusken Valuables collection, providing context for its significance in the game. Overall, A Demon Doll serves as a notable achievement for players who complete the specified collection. &

\textbf{LLM Summary: \cmark} \newline
The Desert Demon Doll is a toy created by the Tusken Raiders on the planet Tatooine. It is a notable part of Tusken Raider culture and is also classified as a toy and merchandise in the Star Wars universe. \\
\hline

\textbf{Node + Relation: \xmark} \newline
A\_Demon\_Doll \newline
abstract: This is the reward for completing the Tusken Valuables collection. \newline
altLabel: A Demon Doll \newline
label: A Demon Doll \newline
subject: Category:Collection\_rewards \newline
type: Thing &

\textbf{Node + Relation: \xmark} \newline
Desert\_Demon\_Doll \newline
abstract: A Desert Demon Doll was a toy made by Tusken Raiders on Tatooine. \newline
altLabel: [Desert demon doll, Desert Demon Doll] \newline
label: Desert Demon Doll \newline
subject: [Category:Tusken\_Raider\_culture, Category:Toys\_and\_merchandise] \newline
type: swgwiki \\
\hline
\end{tabular}
\caption{Comparison of gold pairs found by LLM summary but not by Node + Relation representations using LLM Selection.}
\label{tab:example-comparison}
\end{table*}

\section{Prompts Used for Top10 Selection}
All prompts formatted the candidate entities between "<EXAMPLE></EXAMPLE>" XML tags, while adding an "ID:\{\}" line to each candidate that the LLM can refer to.  

\subsection{Normal}

\begin{lstlisting}[basicstyle=\ttfamily\footnotesize]
TASK: You will be given a description of an anchor entity and a list of candidate entities, all formatted with <EXAMPLE> tags. Your task is to:
- Identify the candidate entity that is the same as the anchor entity.
- Return the ID number of the matching candidate entity.
- If none of the candidates match the anchor entity, return -1.
- Answer with the ID (or -1) only, no explanations
###
<ANCHOR>
{anchor}
</ANCHOR>
###
{candidates strings}
###
Answer:
\end{lstlisting}

\subsection{Strict}
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize]
TASK: You will be given a description of an anchor entity and a list of candidate entities, all formatted with <EXAMPLE> tags. Your task is to:
- Identify the candidate entity that is the same as the anchor entity.
- Return the ID number of the matching candidate entity. The selected Candidate MUST BE the same as the Anchor entity. 
- If none of the candidates match the anchor entity, return -1.
- Answer with the ID (or -1) only, no explanations
###
<ANCHOR>
{anchor}
</ANCHOR>
###
{candidates strings}
###
Answer:
\end{lstlisting}

2. Deduplication table

3. Best F1 table

\end{document}

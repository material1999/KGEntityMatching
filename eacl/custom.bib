@article{gnn_1,
  author  = {Zonghan Wu and Shirui Pan and Fengwen Chen and Guodong Long and Chengqi Zhang and Philip S. Yu},
  title   = {A Comprehensive Survey on Graph Neural Networks},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  year    = {2021},
  volume  = {32},
  number  = {1},
  pages   = {4-24},
  doi     = {10.1109/TNNLS.2020.2978386}
}

@article{gnn_2,
  author  = {Gabriele Corso and Hannes Stark and Stefanie Jegelka and Tommi Jaakkola and Regina Barzilay},
  title   = {Graph neural networks},
  journal = {Nature Reviews Methods Primers},
  year    = {2024},
  volume  = {4},
  number  = {1},
  pages   = {17},
  doi     = {10.1038/s43586-024-00294-7}
}

@article{sentence_bert,
  author  = {Nils Reimers and Iryna Gurevych},
  title   = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  journal = {arXiv},
  year    = {2019},
  doi     = {10.48550/arXiv.1908.10084}
}

@inproceedings{oaei_kgtrack,
  author    = {Sven Hertling and Heiko Paulheim},
  title     = {The Knowledge Graph Track at OAEI},
  booktitle = {The Semantic Web},
  editor    = {Andreas Harth and Sabrina Kirrane and Axel-Cyrille Ngonga Ngomo and Heiko Paulheim and Anisa Rula and Anna Lisa Gentile and Peter Haase and Michael Cochez},
  pages     = {343--359},
  year      = {2020},
  publisher = {Springer International Publishing},
  address   = {Cham},
  isbn      = {978-3-030-49461-2},
  doi       = {10.1007/978-3-030-49461-2_20}
}

@article{kg_opportunities,
  author    = {Ciyuan Peng and Feng Xia and Mehdi Naseriparsa and Francesco Osborne},
  title     = {Knowledge Graphs: Opportunities and Challenges},
  journal   = {Artificial Intelligence Review},
  volume    = {56},
  number    = {11},
  pages     = {13071--13102},
  year      = {2023},
  issn      = {1573-7462},
  doi       = {10.1007/s10462-023-10465-9}
}

@misc{rag_survey,
  author       = {Shailja Gupta and Rajesh Ranjan and Surya Narayan Singh},
  title        = {A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions},
  year         = {2024},
  eprint       = {2410.12837},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  doi          = {10.48550/arXiv.2410.12837}
}


@misc{graphrag,
  author       = {Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Dasha Metropolitansky and Robert Osazuwa Ness and Jonathan Larson},
  title        = {From Local to Global: A Graph RAG Approach to Query-Focused Summarization},
  year         = {2025},
  eprint       = {2404.16130},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  doi          = {10.48550/arXiv.2404.16130}
}

@article{hl7_fhir,
  author    = {Athanasios Kiourtis and Argyro Mavrogiorgou and Andreas Menychtas and Ilias Maglogiannis and Dimosthenis Kyriazis},
  title     = {Structurally Mapping Healthcare Data to HL7 FHIR through Ontology Alignment},
  journal   = {Journal of Medical Systems},
  volume    = {43},
  number    = {3},
  pages     = {62},
  year      = {2019},
  doi       = {10.1007/s10916-019-1183-y}
}

@inproceedings{personalized_medicine,
  author    = {Marta Contreiras Silva and Daniel Faria and Catia Pesquita},
  title     = {Matching Multiple Ontologies to Build a Knowledge Graph for Personalized Medicine},
  booktitle = {The Semantic Web},
  editor    = {Paul Groth and Maria-Esther Vidal and Fabian Suchanek and Pedro Szekley and Pavan Kapanipathi and Catia Pesquita and Hala Skaf-Molli and Minna Tamper},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {461--477},
  year      = {2022},
  doi       = {10.1007/978-3-031-06981-9_27}
}

@article{logistics_alignment,
  author  = {Majid Mohammadi and Wout Hofman and Yao-Hua Tan},
  title   = {Seamless interoperability in logistics by ontology alignment},
  journal = {Journal of Supply Chain Management Science},
  volume  = {1},
  number  = {3-4},
  pages   = {104--117},
  year    = {2020},
  doi     = {10.18757/jscms.2020.5444}
}

@article{survey_background,
  author  = {Jan Portisch and Michael Hladik and Heiko Paulheim},
  title   = {Background knowledge in ontology matching: A survey},
  journal = {Semantic Web},
  volume  = {15},
  number  = {6},
  pages   = {2639--2693},
  year    = {2024},
  doi     = {10.3233/SW-223085}
}

@article{alignment_survey,
  author  = {Kaisheng Zeng and Chengjiang Li and Lei Hou and Juanzi Li and Ling Feng},
  title   = {A comprehensive survey of entity alignment for knowledge graphs},
  journal = {AI Open},
  volume  = {2},
  pages   = {1--13},
  year    = {2021},
  issn    = {2666-6510},
  doi     = {10.1016/j.aiopen.2021.02.002}
}

@article{matching_survey,
  author  = {Yuliang Li and Jinfeng Li and Yoshihiko Suhara and Jin Wang and Wataru Hirota and Wang-Chiew Tan},
  title   = {Deep Entity Matching: Challenges and Opportunities},
  journal = {J. Data and Information Quality},
  volume  = {13},
  number  = {1},
  pages   = {1--17},
  year    = {2021},
  doi     = {10.1145/3431816}
}

@article{alignment_bertmap,
  author  = {Yuan He and Jiaoyan Chen and Denvar Antonyrajah and Ian Horrocks},
  title   = {BERTMap: A BERT-Based Ontology Alignment System},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume  = {36},
  number  = {5},
  pages   = {5684--5691},
  year    = {2022},
  doi     = {10.1609/aaai.v36i5.20510}
}


@article{alignment_embedding,
  author  = {Beibei Zhu and Ruolin Wang and Junyi Wang and Fei Shao and Kerun Wang},
  title   = {A survey: knowledge graph entity alignment research based on graph embedding},
  journal = {Artificial Intelligence Review},
  volume  = {57},
  number  = {9},
  pages   = {229},
  year    = {2024},
  doi     = {10.1007/s10462-024-10866-4}
}

@article{alignment_llm_1,
  author  = {Yuan He and Jiaoyan Chen and Hang Dong and Ian Horrocks},
  title   = {Exploring Large Language Models for Ontology Alignment},
  journal = {arXiv},
  year    = {2023},
  doi     = {10.48550/arXiv.2309.07172}
}

@article{alignment_llm_2,
  author  = {Hamed Babaei Giglou and Jennifer D'Souza and Felix Engel and SÃ¶ren Auer},
  title   = {LLMs4OM: Matching Ontologies with Large Language Models},
  journal = {arXiv},
  year    = {2024},
  doi     = {10.48550/arXiv.2404.10317}
}

@inproceedings{alignment_llm_3,
  author    = {Reihaneh Amini and Sanaz Saki Norouzi and Pascal Hitzler and Reza Amini},
  title     = {Towards Complex Ontology Alignment Using Large Language Models},
  booktitle = {Knowledge Graphs and Semantic Web},
  year      = {2025},
  doi       = {10.1007/978-3-031-81221-7_2}
}

@article{matching_embedding,
  author    = {Weixin Zeng and Xiang Zhao and Zhen Tan and Jiuyang Tang and Xueqi Cheng},
  title     = {Matching Knowledge Graphs in Entity Embedding Spaces: An Experimental Study},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  year      = {2023},
  volume    = {35},
  number    = {12},
  pages     = {12770--12784},
  doi       = {10.1109/TKDE.2023.3272584}
}

@article{matching_transformers,
  author    = {Yuliang Li and Jinfeng Li and Yoshi Suhara and AnHai Doan and Wang-Chiew Tan},
  title     = {Effective entity matching with transformers},
  journal   = {The VLDB Journal},
  year      = {2023},
  volume    = {32},
  number    = {6},
  pages     = {1215--1235},
  doi       = {10.1007/s00778-023-00779-z}
}

@article{matching_montecarlo,
  author    = {Teon Volkova and Evander Delacruz and Thaddeus Cavanaugh},
  title     = {A Novel Approach to Optimize Large Language Models for Named Entity Matching with Monte Carlo Tree Search},
  journal   = {Wiley},
  year      = {2024},
  doi       = {10.22541/au.172659950.06571592/v1}
}

@inproceedings{matching_chatgpt_survey,
  author    = {Ralph Peeters and Christian Bizer},
  title     = {Using ChatGPT for Entity Matching},
  booktitle = {New Trends in Database and Information Systems},
  editor    = {Alberto Abell{\'o} and Panos Vassiliadis and Oscar Romero and Robert Wrembel and Francesca Bugiotti and Johann Gamper and Genoveva Vargas Solar and Ester Zumpano},
  year      = {2023},
  publisher = {Springer Nature Switzerland},
  address   = {Cham},
  pages     = {221--230},
  doi       = {10.1007/978-3-031-42941-5_20},
  isbn      = {978-3-031-42941-5}
}

@article{matching_llm_survey,
  author       = {Ralph Peeters and Aaron Steiner and Christian Bizer},
  title        = {Entity Matching using Large Language Models},
  journal      = {arXiv},
  year         = {2024},
  doi          = {10.48550/arXiv.2310.11244},
  eprint       = {2310.11244}
}

@misc{matching_zeroshot,
  title        = {AnyMatch -- Efficient Zero-Shot Entity Matching with a Small Language Model},
  author       = {Zeyu Zhang and Paul Groth and Iacer Calixto and Sebastian Schelter},
  year         = {2024},
  eprint       = {2409.04073},
  doi          = {10.48550/arXiv.2409.04073}
}

@misc{matching_multi_decision,
  title        = {Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching},
  author       = {Tianshu Wang and Xiaoyang Chen and Hongyu Lin and Xuanang Chen and Xianpei Han and Hao Wang and Zhenyu Zeng and Le Sun},
  year         = {2024},
  eprint       = {2405.16884},
  doi          = {10.48550/arXiv.2405.16884}
}

@misc{matching_background,
  title        = {Unlocking the Power of Large Language Models for Entity Alignment},
  author       = {Xuhui Jiang and Yinghan Shen and Zhichao Shi and Chengjin Xu and Wei Li and Zixuan Li and Jian Guo and Huawei Shen and Yuanzhuo Wang},
  year         = {2024},
  eprint       = {2402.15048},
  doi          = {10.48550/arXiv.2402.15048}
}

@inproceedings{matching_finetune,
  author    = {Aaron Steiner and Ralph Peeters and Christian Bizer},
  booktitle = {2025 IEEE 41st International Conference on Data Engineering Workshops (ICDEW)},
  title     = {Fine-Tuning Large Language Models for Entity Matching},
  year      = {2025},
  pages     = {9--17},
  doi       = {10.1109/ICDEW67478.2025.00006}
}

@misc{compare_1,
  title     = {Two Heads Are Better Than One: Integrating Knowledge from Knowledge Graphs and Large Language Models for Entity Alignment},
  author    = {Linyao Yang and Hongyang Chen and Xiao Wang and Jing Yang and Fei-Yue Wang and Han Liu},
  year      = {2024},
  eprint    = {2401.16960},
  doi       = {10.48550/arXiv.2401.16960}
}

@inproceedings{compare_2,
  title     = {EasyEA: Large Language Model is All You Need in Entity Alignment Between Knowledge Graphs},
  author    = {Jingwei Cheng and Chenglong Lu and Linyan Yang and Guoqing Chen and Fu Zhang},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2025},
  year      = {2025},
  address   = {Vienna, Austria},
  publisher = {Association for Computational Linguistics},
  pages     = {20981--20995},
  isbn      = {979-8-89176-256-5},
  doi       = {10.18653/v1/2025.findings-acl.1080}
}

@misc{context_survey,
  title        = {A Survey of Context Engineering for Large Language Models},
  author       = {Lingrui Mei and Jiayu Yao and Yuyao Ge and Yiwei Wang and Baolong Bi and Yujun Cai and Jiazhi Liu and Mingyu Li and Zhong-Zhi Li and Duzhen Zhang and Chenlin Zhou and Jiayi Mao and Tianze Xia and Jiafeng Guo and Shenghua Liu},
  year         = {2025},
  eprint       = {2507.13334},
  doi          = {10.48550/arXiv.2507.13334}
}


@article{jiang2023structgpt,
  title={Structgpt: A general framework for large language model to reason over structured data},
  author={Jiang, Jinhao and Zhou, Kun and Dong, Zican and Ye, Keming and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2305.09645},
  year={2023}
}

@article{fang2024karpa,
  title={KARPA: A Training-free Method of Adapting Knowledge Graph as References for Large Language Model's Reasoning Path Aggregation},
  author={Fang, Siyuan and Ma, Kaijing and Zheng, Tianyu and Du, Xinrun and Lu, Ningxuan and Zhang, Ge and Tang, Qingkun},
  journal={arXiv preprint arXiv:2412.20995},
  year={2024}
}

@article{luo2023reasoning_rog,
  title={Reasoning on graphs: Faithful and interpretable large language model reasoning},
  author={Luo, Linhao and Li, Yuan-Fang and Haffari, Gholamreza and Pan, Shirui},
  journal={arXiv preprint arXiv:2310.01061},
  year={2023}
}

@article{sun2023think,
  title={Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph},
  author={Sun, Jiashuo and Xu, Chengjin and Tang, Lumingyuan and Wang, Saizhuo and Lin, Chen and Gong, Yeyun and Ni, Lionel M and Shum, Heung-Yeung and Guo, Jian},
  journal={arXiv preprint arXiv:2307.07697},
  year={2023}
}

@misc{liu2024_struxgpt,
      title={Enhancing LLM's Cognition via Structurization}, 
      author={Kai Liu and Zhihang Fu and Chao Chen and Wei Zhang and Rongxin Jiang and Fan Zhou and Yaowu Chen and Yue Wu and Jieping Ye},
      year={2024},
      eprint={2407.16434},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.16434}, 
}

@inproceedings{ankush-etal-2023-kitlm,
    title = "{KITLM}: Domain-Specific Knowledge {I}n{T}egration into Language Models for Question Answering",
    author = "Agarwal, Ankush  and
      Gawade, Sakharam  and
      Azad, Amar Prakash  and
      Bhattacharyya, Pushpak",
    editor = "D. Pawar, Jyoti  and
      Lalitha Devi, Sobha",
    booktitle = "Proceedings of the 20th International Conference on Natural Language Processing (ICON)",
    month = dec,
    year = "2023",
    address = "Goa University, Goa, India",
    publisher = "NLP Association of India (NLPAI)",
    url = "https://aclanthology.org/2023.icon-1.20/",
    pages = "202--294",
    abstract = "Large language models (LLMs) have demon- strated remarkable performance in a wide range of natural language tasks. However, as these models continue to grow in size, they face sig- nificant challenges in terms of computational costs. Additionally, LLMs often lack efficient domain-specific understanding, which is par- ticularly crucial in specialized fields such as aviation and healthcare. To boost the domain- specific understanding, we propose, KITLM 1 , a novel knowledge base integration approach into language model through relevant informa- tion infusion. By integrating pertinent knowl- edge, not only the performance of the lan- guage model is greatly enhanced, but the model size requirement is also significantly reduced while achieving comparable performance. Our proposed knowledge-infused model surpasses the performance of both GPT-3.5-turbo and the state-of-the-art knowledge infusion method, SKILL, achieving over 1.5 times improvement in exact match scores on the MetaQA. KITLM showed a similar performance boost in the avi- ation domain with AeroQA. The drastic perfor- mance improvement of KITLM over the exist- ing methods can be attributed to the infusion of relevant knowledge while mitigating noise. In addition, we release two curated datasets to accelerate knowledge infusion research in specialized fields: a) AeroQA, a new bench- mark dataset designed for multi-hop question- answering within the aviation domain, and b) Aviation Corpus, a dataset constructed from unstructured text extracted from the National Transportation Safety Board reports. Our re- search contributes to advancing the field of domain-specific language understanding and showcases the potential of knowledge infusion techniques in improving the performance."
}

@inproceedings{agarwal-etal-2021-knowledge,
    title = "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training",
    author = "Agarwal, Oshin  and
      Ge, Heming  and
      Shakeri, Siamak  and
      Al-Rfou, Rami",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.278/",
    doi = "10.18653/v1/2021.naacl-main.278",
    pages = "3554--3565",
    abstract = "Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe."
}

@misc{ge2025graphdescriptiveorderaffect,
      title={Can Graph Descriptive Order Affect Solving Graph Problems with LLMs?}, 
      author={Yuyao Ge and Shenghua Liu and Baolong Bi and Yiwei Wang and Lingrui Mei and Wenjie Feng and Lizhe Chen and Xueqi Cheng},
      year={2025},
      eprint={2402.07140},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.07140}, 
}
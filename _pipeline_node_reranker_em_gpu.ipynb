{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import torch\n",
    "from sentence_transformers import util\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ],
   "id": "6cda31fadd2c0ed6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "small = \"stexpanded\"\n",
    "big = \"memoryalpha\"\n",
    "embeddings = \"dogtag_bgelarge\"\n",
    "top = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "d435aa4153a2dd85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mappings_file_small = \"/hdd/mappings/\" + small + \".json\"\n",
    "mappings_file_big = \"/hdd/mappings/\" + big + \".json\"\n",
    "\n",
    "node_embeddings_small_file = \"/hdd/node_embeddings/\" + embeddings + \"/\" + small + \".json\"\n",
    "node_embeddings_big_file = \"/hdd/node_embeddings/\" + embeddings + \"/\" + big + \".json\"\n",
    "\n",
    "dogtags_small_file = \"/hdd/dogtags/\" + small + \".json\"\n",
    "dogtags_big_file = \"/hdd/dogtags/\" + big + \".json\"\n",
    "\n",
    "exact_match_file = \"./_input/exact_match/\" + small + \"-\" + big + \".json\"\n",
    "\n",
    "output_file = \"/hdd/found_pairs_em/\" + small + \"-\" + big + \".txt\""
   ],
   "id": "98e03d2a85eaf617"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(mappings_file_small) as file:\n",
    "    mappings_small = {str(v): k for k, v in json.load(file).items()}\n",
    "    mappings_small_reversed = {v: k for k, v in mappings_small.items()}\n",
    "\n",
    "with open(mappings_file_big) as file:\n",
    "    mappings_big = {str(v): k for k, v in json.load(file).items()}\n",
    "    mappings_big_reversed = {v: k for k, v in mappings_big.items()}\n",
    "\n",
    "with open(node_embeddings_small_file) as nesf:\n",
    "    node_embeddings_small = json.load(nesf)\n",
    "    node_embeddings_small = {mappings_small_reversed[k]: v for k, v in node_embeddings_small.items()}\n",
    "\n",
    "with open(node_embeddings_big_file) as nebf:\n",
    "    node_embeddings_big = json.load(nebf)\n",
    "    node_embeddings_big = {mappings_big_reversed[k]: v for k, v in node_embeddings_big.items()}\n",
    "\n",
    "with open(dogtags_small_file) as df:\n",
    "    dogtags_small = json.load(df)\n",
    "\n",
    "with open(dogtags_big_file) as df:\n",
    "    dogtags_big = json.load(df)\n",
    "\n",
    "with open(exact_match_file) as file:\n",
    "    exact_match = json.load(file)"
   ],
   "id": "9b2b75351d96e381"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "node_embeddings_small_list = list()\n",
    "node_ids_small_list = list()\n",
    "\n",
    "node_embeddings_big_list = list()\n",
    "node_ids_big_list = list()\n",
    "\n",
    "for k, v in node_embeddings_small.items():\n",
    "    node_ids_small_list.append(k)\n",
    "    node_embeddings_small_list.append(v)\n",
    "\n",
    "for k, v in node_embeddings_big.items():\n",
    "    node_ids_big_list.append(k)\n",
    "    node_embeddings_big_list.append(v)\n",
    "\n",
    "with open(exact_match_file) as file:\n",
    "    exact_match = json.load(file)"
   ],
   "id": "dc64d1bbbd591c93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tensor_small = torch.Tensor(node_embeddings_small_list).to(device)\n",
    "tensor_big = torch.Tensor(node_embeddings_big_list).to(device)\n",
    "node_order = util.semantic_search(tensor_small, tensor_big, top_k=top)"
   ],
   "id": "4b5d615179f9bd18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "top_dict = dict()\n",
    "for idx, (node_id, order) in enumerate(zip(node_ids_small_list, node_order)):\n",
    "    items_list = list()\n",
    "    for item in order:\n",
    "        items_list.append((node_ids_big_list[item['corpus_id']], item['score']))\n",
    "    top_dict[node_id] = items_list"
   ],
   "id": "d5f56271ebd1b047"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for em in exact_match:\n",
    "    if str(em[0]) in top_dict:\n",
    "        bool_append = True\n",
    "        for item in top_dict[str(em[0])]:\n",
    "            if item[0] == str(em[1]):\n",
    "                bool_append = False\n",
    "                break\n",
    "        if bool_append:\n",
    "            top_dict[str(em[0])].append((str(em[1]), 1.0))"
   ],
   "id": "d588b64eac26cf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-large')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-large')\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "id": "c0be466c70d494c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(output_file, \"w\") as file:\n",
    "    for node in node_ids_small_list:\n",
    "        id_list = list()\n",
    "        str_list = list()\n",
    "        for i in range(0, top):\n",
    "            id_list.append(top_dict[node][i][0])\n",
    "            str_list.append(\n",
    "                [\n",
    "                    str(dogtags_small[mappings_small[node]]),\n",
    "                    str(dogtags_big[mappings_big[top_dict[node][i][0]]])\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        pairs = str_list\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512).to(device)\n",
    "            scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "            # print(scores)\n",
    "\n",
    "        max_index = torch.argmax(scores)\n",
    "        max_index_int = int(max_index.item())\n",
    "        max_value = scores[max_index]\n",
    "        max_value_float = float(max_value.item())\n",
    "\n",
    "        file.write(\n",
    "            mappings_small[node] + \"###\" +\n",
    "            mappings_big[id_list[max_index_int]] + \"###\" +\n",
    "            str(max_value_float) + \"\\n\"\n",
    "        )\n",
    "        file.flush()"
   ],
   "id": "a75f1832c3423e42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "540b69412856b466"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
